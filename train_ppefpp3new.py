"""
Training Script for UNet_PPEF++
ä½¿ç”¨PPEF++æ¡†æ¶è®­ç»ƒé«˜å…‰è°±å›¾åƒåˆ†å‰²æ¨¡å‹

ä½¿ç”¨è¯´æ˜ï¼š
1. ç¡®ä¿æ‰€æœ‰PPEF++æ¨¡å—å·²æ­£ç¡®å®‰è£…
2. ä¿®æ”¹æ•°æ®è·¯å¾„å’Œå‚æ•°é…ç½®
3. è¿è¡Œï¼špython train_ppefpp.py --fold 0

ä¸»è¦æ”¹åŠ¨ï¼š
- ä½¿ç”¨UNet_PPEFPPæ¨¡å‹ï¼ˆæ›¿ä»£UNetï¼‰
- ä½¿ç”¨PPEFPPLossæŸå¤±å‡½æ•°
- åœ¨è®­ç»ƒå¾ªç¯ä¸­è·å–unmixingè¾“å‡º
- è®°å½•é¢å¤–çš„æŸå¤±é¡¹ï¼ˆé‡å»ºã€å¹³æ»‘ã€å¤šæ ·æ€§ã€ç†µï¼‰
"""

import os
import sys
import torch
import argparse
import json
import time
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from datetime import datetime
from torch.utils.data import DataLoader
import torch.optim.lr_scheduler as lr_scheduler
from torch.amp import autocast
from torch.cuda.amp import GradScaler

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥train.py
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# ä»train.pyå¯¼å…¥éœ€è¦çš„ç±»å’Œå‡½æ•°
from train import (
    HyperspectralPresetTrain,
    EMA
)
from dataset import HyperspectralDatasetWithKeys, HyperspectralPatchDataset
from utils import calculate_metrics, sliding_window_predict

# å¯¼å…¥PPEF++æ¨¡å—
from model_ppefpp import UNet_PPEFPP
from loss_ppefpp import PPEFPPLoss


def train_one_epoch_ppefpp(model, criterion, optim, train_loader, device, scaler=None, epoch=None):
    """
    è®­ç»ƒä¸€ä¸ªepochï¼ˆPPEF++ç‰ˆæœ¬ï¼‰
    
    Args:
        epoch: å½“å‰epochï¼ˆç”¨äºwarmupï¼‰
    
    Returns:
        avg_loss: å¹³å‡æ€»æŸå¤±
        loss_details: å„é¡¹æŸå¤±çš„å¹³å‡å€¼
        lr: å½“å‰å­¦ä¹ ç‡
    """
    model.train()
    
    total_loss = 0.0
    loss_details = {}
    
    for batch_idx, (images, targets, _) in enumerate(train_loader):
        images = images.to(device)
        targets = targets.to(device)
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if scaler is not None:
            with autocast(device_type='cuda'):
                # å‰å‘ä¼ æ’­ï¼ˆç›´æ¥è¿”å›unmixingè¾“å‡ºï¼‰
                outputs, A2, X2_hat, X2_down = model(images, return_unmixing=True)
                prototypes = model.get_prototypes(to_cpu=False)  # âœ… ä¿æŒåœ¨GPUä¸Š
                
                # è®¡ç®—æŸå¤±ï¼ˆç«‹å³ä½¿ç”¨ï¼Œç”¨å®Œå³é‡Šæ”¾ï¼‰
                loss, loss_dict = criterion(
                    outputs, targets,
                    A2=A2, X2_hat=X2_hat, X2_down=X2_down,
                    prototypes=prototypes,
                    epoch=epoch  # ä¼ å…¥epochç”¨äºwarmup
                )
            
            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.step(optim)
            scaler.update()
            optim.zero_grad(set_to_none=True)  # âœ… ä¼˜åŒ–ï¼šæ¸…é›¶æ¢¯åº¦å¹¶é‡Šæ”¾å†…å­˜
        else:
            # å‰å‘ä¼ æ’­ï¼ˆç›´æ¥è¿”å›unmixingè¾“å‡ºï¼‰
            outputs, A2, X2_hat, X2_down = model(images, return_unmixing=True)
            prototypes = model.get_prototypes(to_cpu=False)  # âœ… ä¿æŒåœ¨GPUä¸Š
            
            # è®¡ç®—æŸå¤±ï¼ˆç«‹å³ä½¿ç”¨ï¼Œç”¨å®Œå³é‡Šæ”¾ï¼‰
            loss, loss_dict = criterion(
                outputs, targets,
                A2=A2, X2_hat=X2_hat, X2_down=X2_down,
                prototypes=prototypes
            )
            
            # åå‘ä¼ æ’­
            loss.backward()
            optim.step()
            optim.zero_grad(set_to_none=True)  # âœ… ä¼˜åŒ–ï¼šæ¸…é›¶æ¢¯åº¦å¹¶é‡Šæ”¾å†…å­˜
        
        total_loss += loss.item()
        
        # ç´¯ç§¯å„é¡¹æŸå¤±ï¼ˆè·³è¿‡étensorå€¼ï¼Œå¦‚warmupæ ‡å¿—ï¼‰
        for key, value in loss_dict.items():
            if isinstance(value, torch.Tensor):  # åªå¤„ç†tensor
                if key not in loss_details:
                    loss_details[key] = 0.0
                loss_details[key] += value.item()
    
    # è®¡ç®—å¹³å‡
    avg_loss = total_loss / len(train_loader)
    for key in loss_details:
        loss_details[key] /= len(train_loader)
    
    # è·å–å­¦ä¹ ç‡
    lr = optim.param_groups[0]['lr']
    
    return avg_loss, loss_details, lr


def calculate_val_loss_and_dice_ppefpp_sliding(model, val_loader, device, patch_size=256, overlap=0.5):
    """
    ä½¿ç”¨æ»‘çª—é¢„æµ‹è®¡ç®—éªŒè¯é›†ä¸Šçš„æŸå¤±å’ŒDiceï¼ˆPPEF++ç‰ˆæœ¬ï¼‰
    
    ç”¨äºpatchè®­ç»ƒæ¨¡å¼ï¼šéªŒè¯æ—¶ä½¿ç”¨å®Œæ•´å›¾åƒï¼Œä½†é€šè¿‡æ»‘çª—æ–¹å¼é¢„æµ‹é¿å…æ˜¾å­˜çˆ†ç‚¸
    
    Args:
        model: PPEF++æ¨¡å‹
        val_loader: éªŒè¯é›†DataLoaderï¼ˆå®Œæ•´å›¾åƒï¼‰
        device: è®¾å¤‡
        patch_size: æ»‘çª—patchå°ºå¯¸
        overlap: é‡å æ¯”ä¾‹ï¼ˆ0.5è¡¨ç¤º50%é‡å ï¼‰
    
    Returns:
        avg_val_loss: éªŒè¯æŸå¤±
        avg_val_dice: éªŒè¯Dice
        loss_details: å„é¡¹æŸå¤±çš„å¹³å‡å€¼
    """
    from utils import sliding_window_predict
    import torch.nn.functional as F
    
    model.eval()
    total_loss = 0.0
    total_dice = 0.0
    n = 0
    
    with torch.no_grad():
        for images, targets, _ in val_loader:
            # images: (B, C, H, W), targets: (B, H, W)
            B = images.shape[0]
            
            for i in range(B):
                img = images[i]  # (C, H, W)
                gt = targets[i].to(device).float()  # (H, W)
                
                # æ»‘çª—é¢„æµ‹ï¼ˆä½¿ç”¨é‡å æ»‘çª—+é«˜æ–¯æƒé‡èåˆï¼‰
                # æ³¨æ„ï¼šsliding_window_predictä¼šè‡ªåŠ¨å¤„ç†return_unmixing=False
                prob_map, pred_mask = sliding_window_predict(
                    model, img, device,
                    patch_size=patch_size,
                    overlap=overlap
                )
                # prob_map: (H, W) æ¦‚ç‡å›¾
                # pred_mask: (H, W) é¢„æµ‹mask {0, 1}
                
                # è®¡ç®—Dice
                pred_flat = pred_mask.float()
                gt_flat = gt.float()
                
                smooth = 1e-5
                intersection = (pred_flat * gt_flat).sum()
                dice = (2. * intersection + smooth) / (pred_flat.sum() + gt_flat.sum() + smooth)
                
                total_dice += dice.item()
                n += 1
                
                # è®¡ç®—Lossï¼ˆåªè®¡ç®—åˆ†å‰²æŸå¤±ï¼Œä¸è®¡ç®—unmixingæŸå¤±ï¼‰
                # ä½¿ç”¨prob_mapç›´æ¥è®¡ç®—BCEæŸå¤±
                prob_map_expanded = prob_map.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)
                gt_expanded = gt.unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)
                
                # BCEæŸå¤±ï¼ˆæ³¨æ„ï¼šprob_mapæ˜¯sigmoidåçš„å€¼ï¼‰
                bce_val = F.binary_cross_entropy(prob_map_expanded, gt_expanded)
                
                # DiceæŸå¤±
                dice_loss_val = 1 - dice
                
                # æ€»æŸå¤±ï¼ˆBCE + Diceï¼‰
                loss = 0.5 * bce_val + 0.5 * dice_loss_val
                total_loss += loss.item()
    
    avg_val_loss = total_loss / max(n, 1)
    avg_val_dice = total_dice / max(n, 1)
    
    # æŸå¤±è¯¦æƒ…ï¼ˆåªæœ‰åˆ†å‰²æŸå¤±ï¼‰
    loss_details = {
        'seg': avg_val_loss
    }
    
    return avg_val_loss, avg_val_dice, loss_details


# ============================================================================
# è®­ç»ƒæœŸé—´çš„å¯è§†åŒ–è¾…åŠ©å‡½æ•° - ç«¯å…ƒå’Œä¸“å®¶æƒé‡æ¼”åŒ–è¿½è¸ª
# ============================================================================

def extract_endmember_info(model):
    """
    æå–ç«¯å…ƒçš„å…³é”®ä¿¡æ¯ï¼ˆç”¨äºæ¼”åŒ–è¿½è¸ªï¼‰
    
    Returns:
        dict: {
            'spectra': (K, C) numpyæ•°ç»„ - ç«¯å…ƒå…‰è°±,
            'correlation_matrix': (K, K) numpyæ•°ç»„ - ç›¸å…³æ€§çŸ©é˜µ,
            'orthogonality_score': float - æ­£äº¤æ€§æŒ‡æ ‡ï¼ˆéå¯¹è§’çº¿ç»å¯¹å€¼å‡å€¼ï¼‰
        }
    """
    import torch.nn.functional as F
    
    # å¤„ç†torch.compile()åŒ…è£…åçš„æ¨¡å‹
    actual_model = model._orig_mod if hasattr(model, '_orig_mod') else model
    
    # æå–ç«¯å…ƒå…‰è°±ï¼ˆæ³¨æ„ï¼šæ¨¡å‹ä¸­çš„å±æ€§åæ˜¯ unmix_headï¼Œä¸æ˜¯ unmixing_headï¼‰
    P = actual_model.unmix_head.P_spec.detach().cpu()  # (K, num_bands)
    
    # å½’ä¸€åŒ–
    P_norm = F.normalize(P, dim=1)
    
    # è®¡ç®—ç›¸å…³çŸ©é˜µ
    corr_matrix = torch.matmul(P_norm, P_norm.T).numpy()  # (K, K)
    
    # è®¡ç®—æ­£äº¤æ€§æŒ‡æ ‡ï¼ˆéå¯¹è§’çº¿ç»å¯¹å€¼å‡å€¼ï¼Œè¶Šå°è¶Šå¥½ï¼‰
    K = corr_matrix.shape[0]
    mask = ~np.eye(K, dtype=bool)
    off_diag = corr_matrix[mask]
    orthogonality_score = np.abs(off_diag).mean()
    
    return {
        'spectra': P.numpy(),  # (K, C)
        'correlation_matrix': corr_matrix,  # (K, K)
        'orthogonality_score': orthogonality_score
    }


def _entropy_norm(W: np.ndarray, eps: float = 1e-12) -> np.ndarray:
    """
    W: (N, E) æ¦‚ç‡åˆ†å¸ƒ
    è¿”å›: (N,) å½’ä¸€åŒ–ç†µ H/log(E)ï¼ŒèŒƒå›´[0,1]
    æ³¨æ„ï¼šç†µå¿…é¡»æŒ‰"æ¯ä¸ªæ ·æœ¬"è®¡ç®—ï¼Œå†åšå‡å€¼/æ–¹å·®ï¼ˆä¸èƒ½ç”¨å¹³å‡æƒé‡ç®—ç†µï¼‰
    """
    E = W.shape[1]
    if E <= 1:
        return np.zeros((W.shape[0],), dtype=np.float32)
    W = np.clip(W, eps, 1.0)
    H = -np.sum(W * np.log(W), axis=1)      # (N,)
    return (H / np.log(E)).astype(np.float32)


def extract_expert_weights_on_monitor(model, monitor_loader, device):
    """
    å›ºå®š Monitor Set ä¸Šæå–ï¼š
    - avg_weights[module] = (E,)
    - stats[module] = {
        entropy_mean, entropy_std,
        top1_rate (E,),
        w_std (E,)
      }
    
    Returns:
        tuple: (avg_weights, stats)
    """
    model.eval()
    
    # torch.compile() å…¼å®¹ï¼šç”¨åŸå§‹æ¨¡å‹è·‘ forwardï¼Œé¿å… attribute ä¿å­˜å¼‚å¸¸
    actual_model = model._orig_mod if hasattr(model, '_orig_mod') else model
    actual_model.eval()
    
    # 6ä¸ªCSSEæ¨¡å—
    csse_modules = {
        'Enc_X2': actual_model.csse_enc1,
        'Enc_X3': actual_model.csse_enc2,
        'Enc_X4': actual_model.csse_enc3,
        'Dec_Up1': actual_model.csse_dec1,
        'Dec_Up2': actual_model.csse_dec2,
        'Dec_Up3': actual_model.csse_dec3,
    }
    
    # é»˜è®¤æ‰€æœ‰å±‚æ˜¯åŒæ ·çš„ä¸“å®¶æ•°ï¼ˆå½“å‰é…ç½®ï¼š2ä¸ªä¸“å®¶ï¼‰ï¼Œè¿™é‡Œåšå¼ºä¸€è‡´æ€§æ ¡éªŒï¼Œé˜²æ­¢â€œæŸå±‚é…ç½®è·‘åâ€
    num_experts = list(csse_modules.values())[0].num_active_experts
    for k, m in csse_modules.items():
        if m.num_active_experts != num_experts:
            raise RuntimeError(f"[CSSE] num_active_experts mismatch at {k}: {m.num_active_experts} vs {num_experts}")
    
    
    per_module_W = {name: [] for name in csse_modules.keys()}
    
    with torch.inference_mode():
        for images, _, _ in monitor_loader:
            img = images.to(device, non_blocking=True)  # (1, C, H, W)
            
            # forward ä¸€æ¬¡ï¼Œè®©å„ CSSE router åœ¨ eval ä¸‹å†™å…¥ last_weights
            _ = actual_model(img, return_unmixing=False)
            
            for name, csse_module in csse_modules.items():
                if csse_module.router is None:
                    # åªæœ‰ä¸€ä¸ªä¸“å®¶ï¼šone-hot
                    w = np.zeros(num_experts, dtype=np.float32)
                    w[0] = 1.0
                    per_module_W[name].append(w)
                    continue
                
                lw = csse_module.router.last_weights
                if lw is None:
                    # fallbackï¼šå‡åŒ€ï¼ˆå°½é‡ä¸è¦é¢‘ç¹å‡ºç°ï¼‰
                    w = np.full(num_experts, 1.0 / num_experts, dtype=np.float32)
                    per_module_W[name].append(w)
                    continue
                
                if isinstance(lw, np.ndarray):
                    w = lw[0].astype(np.float32)  # (E,)
                else:
                    w = lw[0].detach().float().cpu().numpy().astype(np.float32)
                
                # å¼ºæ ¡éªŒï¼šç»´åº¦ + å½’ä¸€
                if w.shape[0] != num_experts:
                    raise RuntimeError(f"[{name}] weight dim mismatch: got {w.shape[0]} vs expected {num_experts}")
                s = float(w.sum())
                if abs(s - 1.0) > 1e-3:
                    raise RuntimeError(f"[{name}] weights not normalized: sum={s:.6f}")
                
                per_module_W[name].append(w)
    
    # æ±‡æ€» avg + stats
    avg_weights = {}
    stats = {}
    
    for name, w_list in per_module_W.items():
        W = np.stack(w_list, axis=0)          # (N,E)
        avg_weights[name] = W.mean(axis=0)    # (E,)
        
        # ç†µï¼šæŒ‰æ ·æœ¬ç®— -> å‡å€¼/æ–¹å·®
        Hn = _entropy_norm(W)                 # (N,)
        # top1ï¼šæŒ‰æ ·æœ¬argmaxç»Ÿè®¡é¢‘ç‡
        top1 = W.argmax(axis=1)               # (N,)
        top1_rate = np.bincount(top1, minlength=num_experts) / len(top1)
        
        stats[name] = {
            "entropy_mean": float(Hn.mean()),
            "entropy_std":  float(Hn.std()),
            "top1_rate":    top1_rate.astype(float).tolist(),
            "w_std":        W.std(axis=0).astype(float).tolist()
        }
    
    return avg_weights, stats


def plot_endmember_evolution(evolution_data, save_path, wavelengths=None):
    """
    ç»˜åˆ¶ç«¯å…ƒæ¼”åŒ–å›¾
    
    Args:
        evolution_data: list of dictï¼Œæ¯ä¸ªdictåŒ…å«ä¸€ä¸ªepochçš„ç«¯å…ƒä¿¡æ¯
        save_path: ä¿å­˜è·¯å¾„
        wavelengths: æ³¢é•¿æ•°ç»„ï¼ˆå¯é€‰ï¼‰
    """
    epochs = [d['epoch'] for d in evolution_data]
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. æ­£äº¤æ€§æŒ‡æ ‡æ¼”åŒ–
    ax = axes[0, 0]
    orth_scores = [d['orthogonality_score'] for d in evolution_data]
    ax.plot(epochs, orth_scores, marker='o', linewidth=2, color='#E74C3C')
    ax.set_xlabel('Epoch', fontsize=11)
    ax.set_ylabel('Orthogonality Score\n(lower is better)', fontsize=11)
    ax.set_title('Endmember Orthogonality Evolution', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.axhline(y=0.1, color='green', linestyle='--', alpha=0.5, label='Good (<0.1)')
    ax.legend()
    
    # 2. ç«¯å…ƒå…‰è°±æ¼”åŒ–ï¼ˆæ˜¾ç¤ºåˆå§‹å’Œæœ€ç»ˆï¼‰
    ax = axes[0, 1]
    K = evolution_data[0]['spectra'].shape[0]
    num_bands = evolution_data[0]['spectra'].shape[1]
    x_axis = wavelengths if wavelengths is not None else np.arange(num_bands)
    xlabel = 'Wavelength (nm)' if wavelengths is not None else 'Band Index'
    
    colors = ['#E74C3C', '#3498DB', '#2ECC71', '#F39C12']
    # æ˜¾ç¤ºåˆå§‹çŠ¶æ€ï¼ˆè™šçº¿ï¼‰
    for k in range(K):
        ax.plot(x_axis, evolution_data[0]['spectra'][k], 
                linestyle='--', color=colors[k], alpha=0.3, label=f'E{k+1} (Init)')
    # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€ï¼ˆå®çº¿ï¼‰
    for k in range(K):
        ax.plot(x_axis, evolution_data[-1]['spectra'][k], 
                linestyle='-', color=colors[k], linewidth=2, label=f'E{k+1} (Final)')
    
    ax.set_xlabel(xlabel, fontsize=11)
    ax.set_ylabel('Reflectance', fontsize=11)
    ax.set_title('Endmember Spectra: Initial vs Final', fontsize=12, fontweight='bold')
    ax.legend(ncol=2, fontsize=8)
    ax.grid(True, alpha=0.3)
    
    # 3. ç›¸å…³æ€§çŸ©é˜µæ¼”åŒ–ï¼ˆæ˜¾ç¤ºåˆå§‹ï¼‰
    ax = axes[1, 0]
    corr_init = evolution_data[0]['correlation_matrix']
    im = ax.imshow(corr_init, cmap='RdBu_r', vmin=-1, vmax=1)
    ax.set_title(f'Correlation Matrix (Epoch {epochs[0]})', fontsize=11, fontweight='bold')
    ax.set_xticks(range(K))
    ax.set_yticks(range(K))
    ax.set_xticklabels([f'E{i+1}' for i in range(K)])
    ax.set_yticklabels([f'E{i+1}' for i in range(K)])
    plt.colorbar(im, ax=ax, fraction=0.046)
    
    # 4. ç›¸å…³æ€§çŸ©é˜µæ¼”åŒ–ï¼ˆæ˜¾ç¤ºæœ€ç»ˆï¼‰
    ax = axes[1, 1]
    corr_final = evolution_data[-1]['correlation_matrix']
    im = ax.imshow(corr_final, cmap='RdBu_r', vmin=-1, vmax=1)
    ax.set_title(f'Correlation Matrix (Epoch {epochs[-1]})', fontsize=11, fontweight='bold')
    ax.set_xticks(range(K))
    ax.set_yticks(range(K))
    ax.set_xticklabels([f'E{i+1}' for i in range(K)])
    ax.set_yticklabels([f'E{i+1}' for i in range(K)])
    plt.colorbar(im, ax=ax, fraction=0.046)
    
    plt.suptitle('Endmember Evolution During Training', fontsize=14, fontweight='bold')
    plt.tight_layout(rect=[0, 0, 1, 0.97])
    plt.savefig(save_path, dpi=200, bbox_inches='tight')
    plt.close()
    
    print(f"âœ“ ç«¯å…ƒæ¼”åŒ–å›¾å·²ä¿å­˜: {save_path}")


def plot_expert_weights_evolution(evolution_data, save_path):
    """
    ç»˜åˆ¶ä¸“å®¶æƒé‡æ¼”åŒ–å›¾
    
    Args:
        evolution_data: list of dictï¼Œæ¯ä¸ªdictåŒ…å«ä¸€ä¸ªepochçš„ä¸“å®¶æƒé‡
        save_path: ä¿å­˜è·¯å¾„
    """
    epochs = [d['epoch'] for d in evolution_data]
    
    # åŠ¨æ€ä¸“å®¶ç³»ç»Ÿï¼ˆæ ¹æ®å®é™…å¯ç”¨çš„ä¸“å®¶æ•°é‡ï¼‰
    if len(evolution_data) > 0 and 'expert_weights' in evolution_data[0]:
        first_weights = list(evolution_data[0]['expert_weights'].values())[0]
        num_experts = len(first_weights)
    else:
        num_experts = 2  # é»˜è®¤2ä¸“å®¶
    
    # å½“å‰é…ç½®ï¼šä¸“å®¶1+2ï¼ˆç¦ç”¨äº†ä¸“å®¶3å’Œ4ï¼‰
    expert_names = ['Expert 1 (Spectral-Local)', 'Expert 2 (Spectral-Global)']
    colors = ['#FF6B6B', '#4ECDC4']
    
    # 6ä¸ªæ¨¡å—
    module_names = ['Enc_X2', 'Enc_X3', 'Enc_X4', 'Dec_Up1', 'Dec_Up2', 'Dec_Up3']
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    for i, module_name in enumerate(module_names):
        ax = axes[i]
        
        # æå–è¯¥æ¨¡å—çš„æƒé‡æ¼”åŒ–
        weights_over_time = []
        for d in evolution_data:
            if module_name in d['expert_weights']:
                weights_over_time.append(d['expert_weights'][module_name])
        
        if len(weights_over_time) == 0:
            continue
        
        weights_array = np.array(weights_over_time)  # (num_epochs, num_experts)
        
        # åŠ¨æ€ç»˜åˆ¶æ›²çº¿ï¼ˆæ ¹æ®å®é™…ä¸“å®¶æ•°é‡ï¼‰
        for j in range(min(weights_array.shape[1], len(expert_names))):
            ax.plot(epochs[:len(weights_over_time)], weights_array[:, j], 
                   marker='o', label=expert_names[j], color=colors[j], linewidth=2)
        
        ax.set_xlabel('Epoch', fontsize=11)
        ax.set_ylabel('Expert Weight', fontsize=11)
        ax.set_title(f'{module_name}', fontsize=12, fontweight='bold')
        ax.set_ylim(0, 1.0)
        uniform_weight = 1.0 / num_experts
        ax.axhline(y=uniform_weight, color='gray', linestyle='--', alpha=0.3, label=f'Uniform ({uniform_weight:.2f})')
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
    
    plt.suptitle('Expert Weights Evolution During Training', fontsize=14, fontweight='bold')
    plt.tight_layout(rect=[0, 0, 1, 0.97])
    plt.savefig(save_path, dpi=200, bbox_inches='tight')
    plt.close()
    
    print(f"âœ“ ä¸“å®¶æƒé‡æ¼”åŒ–å›¾å·²ä¿å­˜: {save_path}")


def save_endmember_data(evolution_data, save_dir):
    """
    ä¿å­˜ç«¯å…ƒæ¼”åŒ–æ•°æ®åˆ°CSVå’ŒJSONæ–‡ä»¶
    
    Args:
        evolution_data: list of dictï¼Œæ¯ä¸ªdictåŒ…å«ä¸€ä¸ªepochçš„ç«¯å…ƒä¿¡æ¯
        save_dir: ä¿å­˜ç›®å½•
    """
    # 1. ä¿å­˜ç«¯å…ƒæ­£äº¤æ€§æ¼”åŒ– (CSV)
    orth_data = {
        'epoch': [d['epoch'] for d in evolution_data],
        'orthogonality_score': [d['orthogonality_score'] for d in evolution_data]
    }
    df_orth = pd.DataFrame(orth_data)
    orth_path = f'{save_dir}/endmember_orthogonality.csv'
    df_orth.to_csv(orth_path, index=False)
    print(f"âœ“ ç«¯å…ƒæ­£äº¤æ€§æ•°æ®å·²ä¿å­˜: {orth_path}")
    
    # 2. ä¿å­˜ç«¯å…ƒå…‰è°±ï¼ˆåˆå§‹ vs æœ€ç»ˆï¼‰(CSV)
    # æ ¼å¼ï¼šendmember, band_idx, epoch, reflectance
    spectra_records = []
    K = evolution_data[0]['spectra'].shape[0]  # ç«¯å…ƒæ•°é‡
    num_bands = evolution_data[0]['spectra'].shape[1]  # æ³¢æ®µæ•°
    
    # åˆå§‹å…‰è°±
    for k in range(K):
        for band_idx in range(num_bands):
            spectra_records.append({
                'endmember': f'E{k+1}',
                'band_idx': band_idx,
                'epoch': evolution_data[0]['epoch'],
                'reflectance': evolution_data[0]['spectra'][k, band_idx]
            })
    
    # æœ€ç»ˆå…‰è°±
    for k in range(K):
        for band_idx in range(num_bands):
            spectra_records.append({
                'endmember': f'E{k+1}',
                'band_idx': band_idx,
                'epoch': evolution_data[-1]['epoch'],
                'reflectance': evolution_data[-1]['spectra'][k, band_idx]
            })
    
    df_spectra = pd.DataFrame(spectra_records)
    spectra_path = f'{save_dir}/endmember_spectra_init_final.csv'
    df_spectra.to_csv(spectra_path, index=False)
    print(f"âœ“ ç«¯å…ƒå…‰è°±æ•°æ®å·²ä¿å­˜: {spectra_path}")
    
    # 3. ä¿å­˜ç«¯å…ƒç›¸å…³æ€§çŸ©é˜µï¼ˆåˆå§‹ vs æœ€ç»ˆï¼‰(JSON)
    corr_data = {
        f"epoch_{evolution_data[0]['epoch']}": {},
        f"epoch_{evolution_data[-1]['epoch']}": {}
    }
    
    # åˆå§‹ç›¸å…³æ€§çŸ©é˜µ
    corr_init = evolution_data[0]['correlation_matrix']
    for i in range(K):
        corr_data[f"epoch_{evolution_data[0]['epoch']}"][f'E{i+1}'] = corr_init[i].tolist()
    
    # æœ€ç»ˆç›¸å…³æ€§çŸ©é˜µ
    corr_final = evolution_data[-1]['correlation_matrix']
    for i in range(K):
        corr_data[f"epoch_{evolution_data[-1]['epoch']}"][f'E{i+1}'] = corr_final[i].tolist()
    
    corr_path = f'{save_dir}/endmember_correlation_init_final.json'
    with open(corr_path, 'w') as f:
        json.dump(corr_data, f, indent=2)
    print(f"âœ“ ç«¯å…ƒç›¸å…³æ€§çŸ©é˜µå·²ä¿å­˜: {corr_path}")
    
    # 4. ã€å¯é€‰ã€‘ä¿å­˜å®Œæ•´çš„ç«¯å…ƒæ¼”åŒ–ï¼ˆæ‰€æœ‰epochï¼‰(NPY)
    # å¦‚æœè®°å½•çš„epochè¾ƒå¤šï¼Œå¯ä»¥ä¿å­˜å®Œæ•´çš„æ¼”åŒ–è¿‡ç¨‹
    if len(evolution_data) > 2:
        spectra_full = np.array([d['spectra'] for d in evolution_data])  # (num_epochs, K, num_bands)
        epochs_full = np.array([d['epoch'] for d in evolution_data])
        
        full_path = f'{save_dir}/endmember_spectra_full.npz'
        np.savez(full_path, 
                 spectra=spectra_full, 
                 epochs=epochs_full,
                 orthogonality_scores=np.array([d['orthogonality_score'] for d in evolution_data]))
        print(f"âœ“ å®Œæ•´ç«¯å…ƒæ¼”åŒ–æ•°æ®å·²ä¿å­˜: {full_path}")


def save_expert_weights_data(evolution_data, save_dir):
    """
    ä¿å­˜ä¸“å®¶æƒé‡æ¼”åŒ–æ•°æ®åˆ°CSVæ–‡ä»¶ï¼ˆåŠ¨æ€é€‚é…ä¸“å®¶æ•°é‡ï¼‰
    
    Args:
        evolution_data: list of dictï¼Œæ¯ä¸ªdictåŒ…å«ä¸€ä¸ªepochçš„ä¸“å®¶æƒé‡
        save_dir: ä¿å­˜ç›®å½•
    """
    # æå–æ•°æ®å¹¶è½¬æ¢ä¸ºCSVæ ¼å¼
    # æ ¼å¼ï¼šepoch, module, expert1_spectral_local, expert2_spectral_global, expert3_spatial_edge
    records = []
    
    module_names = ['Enc_X2', 'Enc_X3', 'Enc_X4', 'Dec_Up1', 'Dec_Up2', 'Dec_Up3']
    # å½“å‰é…ç½®ï¼šä¸“å®¶1+2ï¼ˆexpert3å’Œexpert4å·²ç¦ç”¨ï¼‰
    # æƒé‡æ•°ç»„ç´¢å¼•0å¯¹åº”expert1ï¼Œç´¢å¼•1å¯¹åº”expert2
    expert_col_names = ['expert1_spectral_local', 'expert2_spectral_global', 'expert3_spatial_edge']
    
    for data in evolution_data:
        epoch = data['epoch']
        weights_dict = data['expert_weights']
        
        for module_name in module_names:
            if module_name in weights_dict:
                weights = weights_dict[module_name]  # (num_active_experts,) numpy array
                record = {
                    'epoch': epoch,
                    'module': module_name,
                }
                # åŠ¨æ€æ·»åŠ ä¸“å®¶æƒé‡åˆ—ï¼ˆæ ¹æ®å¯ç”¨çš„ä¸“å®¶æ˜ å°„ï¼‰
                # å½“å‰é…ç½®ï¼šweights[0]=expert1, weights[1]=expert2
                active_experts = ['expert1_spectral_local', 'expert2_spectral_global']
                for i in range(len(weights)):
                    if i < len(active_experts):
                        record[active_experts[i]] = weights[i]
                records.append(record)
    
    df_weights = pd.DataFrame(records)
    weights_path = f'{save_dir}/expert_weights_evolution.csv'
    df_weights.to_csv(weights_path, index=False)
    print(f"âœ“ ä¸“å®¶æƒé‡æ¼”åŒ–æ•°æ®å·²ä¿å­˜: {weights_path}")
    
    # ã€å¯é€‰ã€‘ä¹Ÿä¿å­˜JSONæ ¼å¼ï¼ˆæ›´ç»“æ„åŒ–ï¼ŒåŠ¨æ€é€‚é…ä¸“å®¶æ•°é‡ï¼‰
    json_data = {}
    
    # åŠ¨æ€åˆå§‹åŒ–ä¸“å®¶å­—æ®µï¼ˆæ ¹æ®ç¬¬ä¸€ä¸ªepochçš„å®é™…ä¸“å®¶æ•°é‡ï¼‰
    if len(evolution_data) > 0:
        first_weights_dict = evolution_data[0]['expert_weights']
        first_module = module_names[0]
        if first_module in first_weights_dict:
            num_experts = len(first_weights_dict[first_module])
            # å½“å‰é…ç½®ï¼šåªæœ‰expert1å’Œexpert2å¯ç”¨
            expert_keys = ['expert1_spectral_local', 'expert2_spectral_global'][:num_experts]
        else:
            expert_keys = expert_col_names  # å›é€€åˆ°é»˜è®¤
    else:
        expert_keys = expert_col_names  # å›é€€åˆ°é»˜è®¤
    
    # åˆå§‹åŒ–JSONç»“æ„
    for module_name in module_names:
        json_data[module_name] = {'epochs': []}
        for key in expert_keys:
            json_data[module_name][key] = []
    
    # å¡«å……æ•°æ®
    for data in evolution_data:
        epoch = data['epoch']
        weights_dict = data['expert_weights']
        
        for module_name in module_names:
            if module_name in weights_dict:
                weights = weights_dict[module_name]
                json_data[module_name]['epochs'].append(epoch)
                # åŠ¨æ€æ·»åŠ æƒé‡å€¼
                for i, key in enumerate(expert_keys):
                    if i < len(weights):
                        json_data[module_name][key].append(float(weights[i]))
    
    json_path = f'{save_dir}/expert_weights_evolution.json'
    with open(json_path, 'w') as f:
        json.dump(json_data, f, indent=2)
    print(f"âœ“ ä¸“å®¶æƒé‡æ¼”åŒ–æ•°æ®å·²ä¿å­˜ï¼ˆJSONæ ¼å¼ï¼‰: {json_path}")


def save_expert_stats_data(evolution_data, save_dir):
    """
    è¾“å‡º 3 ä¸ªCSVï¼š
      1) expert_entropy_evolution.csv: epoch,module,entropy_mean,entropy_std,val_dice
      2) expert_top1_rate_evolution.csv: epoch,module,top1_rate_e1..eE
      3) expert_weight_std_evolution.csv: epoch,module,std_e1..eE
    """
    module_names = ['Enc_X2', 'Enc_X3', 'Enc_X4', 'Dec_Up1', 'Dec_Up2', 'Dec_Up3']
    
    entropy_records = []
    top1_records = []
    std_records = []
    
    for d in evolution_data:
        epoch = d.get('epoch', None)
        stats = d.get('expert_stats', None)
        if epoch is None or stats is None:
            continue
        
        val_dice = d.get('val_dice', None)
        
        for m in module_names:
            if m not in stats:
                continue
            
            s = stats[m]
            entropy_records.append({
                'epoch': epoch,
                'module': m,
                'entropy_mean': s['entropy_mean'],
                'entropy_std':  s['entropy_std'],
                'val_dice':     val_dice
            })
            
            # åŠ¨æ€ä¸“å®¶æ•°ï¼ˆä»top1_rateé•¿åº¦æ¨æ–­ï¼‰
            top1_rate = s['top1_rate']
            w_std = s['w_std']
            E = min(len(top1_rate), len(w_std))
            
            r_top1 = {'epoch': epoch, 'module': m}
            r_std  = {'epoch': epoch, 'module': m}
            for i in range(E):
                r_top1[f'top1_rate_e{i+1}'] = float(top1_rate[i])
                r_std[f'std_e{i+1}'] = float(w_std[i])
            
            top1_records.append(r_top1)
            std_records.append(r_std)
    
    entropy_path = os.path.join(save_dir, 'expert_entropy_evolution.csv')
    top1_path    = os.path.join(save_dir, 'expert_top1_rate_evolution.csv')
    std_path     = os.path.join(save_dir, 'expert_weight_std_evolution.csv')
    
    pd.DataFrame(entropy_records).to_csv(entropy_path, index=False)
    pd.DataFrame(top1_records).to_csv(top1_path, index=False)
    pd.DataFrame(std_records).to_csv(std_path, index=False)
    
    print(f"âœ“ Expert stats saved:")
    print(f"  - {entropy_path}")
    print(f"  - {top1_path}")
    print(f"  - {std_path}")


def plot_expert_entropy_evolution(evolution_data, save_path):
    """ç»˜åˆ¶ä¸“å®¶è·¯ç”±ç†µæ¼”åŒ–å›¾"""
    module_names = ['Enc_X2', 'Enc_X3', 'Enc_X4', 'Dec_Up1', 'Dec_Up2', 'Dec_Up3']
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    for i, m in enumerate(module_names):
        ax = axes[i]
        ep, meanH, stdH = [], [], []
        
        for d in evolution_data:
            stats = d.get('expert_stats', None)
            if stats is None or m not in stats:
                continue
            ep.append(d['epoch'])
            meanH.append(stats[m]['entropy_mean'])
            stdH.append(stats[m]['entropy_std'])
        
        if len(ep) == 0:
            continue
        
        meanH = np.array(meanH); stdH = np.array(stdH)
        ax.plot(ep, meanH, marker='o', linewidth=2, color='#E74C3C')
        ax.fill_between(ep, meanH - stdH, meanH + stdH, alpha=0.2, color='#E74C3C')
        
        ax.set_title(f'{m} | Routing Entropy (norm)', fontsize=12, fontweight='bold')
        ax.set_xlabel('Epoch', fontsize=11)
        ax.set_ylabel('H(w)/log(E)', fontsize=11)
        ax.set_ylim(0.0, 1.0)
        ax.grid(True, alpha=0.3)
    
    plt.suptitle('Routing Entropy Evolution (Monitor Set)', fontsize=14, fontweight='bold')
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig(save_path, dpi=200, bbox_inches='tight')
    plt.close()
    print(f"âœ“ Saved: {save_path}")


def plot_expert_top1_evolution(evolution_data, save_path):
    """ç»˜åˆ¶Top-1ä¸“å®¶é€‰æ‹©é¢‘ç‡æ¼”åŒ–å›¾"""
    module_names = ['Enc_X2', 'Enc_X3', 'Enc_X4', 'Dec_Up1', 'Dec_Up2', 'Dec_Up3']
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    colors = ['#FF6B6B', '#4ECDC4', '#FFE66D']
    expert_labels = ['Expert 1 (Spectral-Local)', 'Expert 2 (Spectral-Global)', 'Expert 3 (Spatial-Edge)']
    
    for i, m in enumerate(module_names):
        ax = axes[i]
        ep = []
        rates = []
        
        for d in evolution_data:
            stats = d.get('expert_stats', None)
            if stats is None or m not in stats:
                continue
            ep.append(d['epoch'])
            rates.append(stats[m]['top1_rate'])
        
        if len(ep) == 0:
            continue
        
        # rates: list of (E,) -> (T,E)
        rates = np.array(rates, dtype=np.float32)
        E = rates.shape[1]
        
        for e in range(E):
            label = expert_labels[e] if e < len(expert_labels) else f'Expert {e+1}'
            color = colors[e] if e < len(colors) else None
            ax.plot(ep, rates[:, e], marker='o', linewidth=2, label=label, color=color)
        
        ax.set_title(f'{m} | Top-1 Frequency', fontsize=12, fontweight='bold')
        ax.set_xlabel('Epoch', fontsize=11)
        ax.set_ylabel('Top-1 rate', fontsize=11)
        ax.set_ylim(0.0, 1.0)
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=8)
    
    plt.suptitle('Top-1 Expert Selection Frequency (Monitor Set)', fontsize=14, fontweight='bold')
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig(save_path, dpi=200, bbox_inches='tight')
    plt.close()
    print(f"âœ“ Saved: {save_path}")


def plot_entropy_vs_valdice(evolution_data, save_path):
    """
    æ¯ä¸ªæ¨¡å—ï¼šentropy_mean ä¸ val_dice åŒå›¾å¯¹ç…§ï¼ˆåŒyè½´ï¼‰
    """
    module_names = ['Enc_X2', 'Enc_X3', 'Enc_X4', 'Dec_Up1', 'Dec_Up2', 'Dec_Up3']
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    for i, m in enumerate(module_names):
        ax = axes[i]
        ep, ent, dice = [], [], []
        
        for d in evolution_data:
            stats = d.get('expert_stats', None)
            if stats is None or m not in stats:
                continue
            if 'val_dice' not in d:
                continue
            
            ep.append(d['epoch'])
            ent.append(stats[m]['entropy_mean'])
            dice.append(d['val_dice'])
        
        if len(ep) == 0:
            continue
        
        # å·¦è½´ï¼šentropy
        line1 = ax.plot(ep, ent, marker='o', linewidth=2, color='#E74C3C', label='Entropy')
        ax.set_title(f'{m} | Entropy vs Val Dice', fontsize=12, fontweight='bold')
        ax.set_xlabel('Epoch', fontsize=11)
        ax.set_ylabel('Entropy (H/logE)', fontsize=11, color='#E74C3C')
        ax.tick_params(axis='y', labelcolor='#E74C3C')
        ax.set_ylim(0.0, 1.0)
        ax.grid(True, alpha=0.3)
        
        # å³è½´ï¼šval_dice
        ax2 = ax.twinx()
        line2 = ax2.plot(ep, dice, marker='s', linewidth=2, linestyle='--', color='#3498DB', label='Val Dice')
        ax2.set_ylabel('Val Dice', fontsize=11, color='#3498DB')
        ax2.tick_params(axis='y', labelcolor='#3498DB')
        ax2.set_ylim(0.0, 1.0)
        
        # åˆå¹¶å›¾ä¾‹
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax.legend(lines, labels, loc='lower right', fontsize=9)
    
    plt.suptitle('Routing Entropy vs Val Dice (Monitor checkpoints)', fontsize=14, fontweight='bold')
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig(save_path, dpi=200, bbox_inches='tight')
    plt.close()
    print(f"âœ“ Saved: {save_path}")


def main(args):
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else "cpu")
    print(f"Using {device} device training.")
    
    # åˆ›å»ºç»“æœç›®å½•
    work_dir = '/data/CXY/gwj/WUnet/2DIM/appefnewfull'
    results_dir = f'{work_dir}/run_results_ppefpp'
    fold_dir = f'{results_dir}/fold_{args.fold}'
    os.makedirs(fold_dir, exist_ok=True)
    
    print(f"Fold {args.fold} ç»“æœå°†ä¿å­˜åˆ°: {fold_dir}\n")
    
    # ä¿å­˜é…ç½®
    with open(f'{fold_dir}/training_config.json', 'w') as f:
        json.dump(vars(args), f, indent=2)
    
    # æ•°æ®å¢å¼º
    if args.patch_size is not None:
        train_tf = HyperspectralPresetTrain(
            rotation_prob=0.5,
            channel_dropout_prob=0.3,
            noise_prob=0.3,
            brightness_prob=0.3
        )
    else:
        train_tf = HyperspectralPresetTrain()
    
    # åŠ è½½æ•°æ®åˆ’åˆ†
    splits_file = '/data/CXY/gwj/WUnet/2DIM/splits_im_patch.json' if args.patch_size else '/data/CXY/gwj/WUnet/2DIM/splits_im.json'
    
    with open(splits_file, 'r') as f:
        splits_data = json.load(f)
    
    if isinstance(splits_data, dict) and 'splits' in splits_data:
        splits_list = splits_data['splits']
    else:
        splits_list = splits_data
    
    train_keys = splits_list[args.fold]['train']
    val_keys = splits_list[args.fold]['val']
    
    # åˆ›å»ºæ•°æ®é›†
    image_dir = '/home/ubuntu/dataset_Med/PLGC/IM/IM_HSI_mat'
    mask_dir = '/home/ubuntu/dataset_Med/PLGC/IM/IM_label_mat'
    
    if args.patch_size is not None:
        patch_image_dir = '/data/CXY/gwj/WUnet/2DIM/patches_im'
        patch_mask_dir = '/data/CXY/gwj/WUnet/2DIM/patches_im_label'
        
        trainDataset = HyperspectralPatchDataset(
            patch_image_dir=patch_image_dir,
            patch_mask_dir=patch_mask_dir,
            patch_list=train_keys,
            transform=train_tf
        )
        
        valDataset = HyperspectralDatasetWithKeys(
            image_dir=image_dir,
            mask_dir=mask_dir,
            keys=val_keys
        )
    else:
        trainDataset = HyperspectralDatasetWithKeys(
            image_dir=image_dir,
            mask_dir=mask_dir,
            keys=train_keys,
            transform=train_tf
        )
        
        valDataset = HyperspectralDatasetWithKeys(
            image_dir=image_dir,
            mask_dir=mask_dir,
            keys=val_keys
        )
    
    # DataLoader
    trainLoader = DataLoader(
        trainDataset, 
        batch_size=args.batch_size,
        num_workers=4,
        shuffle=True,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4
    )
    
    # éªŒè¯é›†batch_sizeï¼šæ»‘çª—é¢„æµ‹æ—¶å¯ä»¥ä½¿ç”¨batch_size>1æ¥æå‡DataLoaderæ•ˆç‡
    # è™½ç„¶æ»‘çª—é¢„æµ‹é€å¼ å¤„ç†ï¼Œä½†batch_size>1å¯ä»¥è®©DataLoaderå¹¶è¡ŒåŠ è½½å¤šå¼ å›¾åƒ
    val_batch_size = 2 if args.patch_size is not None else args.batch_size
    
    # éªŒè¯é›†DataLoader
    valLoader = DataLoader(
        valDataset,
        batch_size=val_batch_size,  # âœ… éªŒè¯æ—¶ç”¨batch_size=2ï¼ˆæå‡DataLoaderæ•ˆç‡ï¼‰
        num_workers=4,  # ä¸è®­ç»ƒé›†ä¿æŒä¸€è‡´
        shuffle=False,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4  # ä¸è®­ç»ƒé›†ä¿æŒä¸€è‡´
    )
    
    # ============================================================================
    # âœ… å›ºå®š Monitor Setï¼ˆç”¨äºä¸“å®¶æƒé‡/ä¸°åº¦ç­‰å¯è§£é‡Šæ€§å¯è§†åŒ–ï¼Œè·¨epochä¸¥æ ¼å¯æ¯”ï¼‰
    # ============================================================================
    monitor_n = min(8, len(val_keys))  # å¯ä»¥æ”¹æˆ 5/8/10
    rng = np.random.default_rng(42 + args.fold)  # å›ºå®šç§å­ï¼Œfoldé—´ä¹ŸåŒºåˆ†ä¸€ä¸‹
    monitor_keys = rng.choice(val_keys, size=monitor_n, replace=False).tolist()
    
    # å¯é€‰ï¼šæ’åºè®©è¾“å‡ºæ›´ç¨³å®šï¼ˆä¸å½±å“"å›ºå®šé›†åˆ"ï¼Œåªå½±å“é¡ºåºï¼‰
    monitor_keys = sorted(monitor_keys)
    
    # ä¿å­˜ä¸‹æ¥ï¼Œä¿è¯å¤ç°ä¸è®ºæ–‡ä¸€è‡´
    with open(f'{fold_dir}/monitor_keys.json', 'w') as f:
        json.dump(monitor_keys, f, indent=2)
    
    monitorDataset = HyperspectralDatasetWithKeys(
        image_dir=image_dir,
        mask_dir=mask_dir,
        keys=monitor_keys
    )
    
    monitorLoader = DataLoader(
        monitorDataset,
        batch_size=1,      # âœ… å¼ºçƒˆå»ºè®®1ï¼Œç®€å•ã€ç¨³å®šã€ä¸ä¼šå‡ºç°batchå†…é€‰æ‹©åå·®
        num_workers=2,     # å¯æŒ‰æœºå™¨è°ƒï¼Œ0/2éƒ½è¡Œ
        shuffle=False,     # âœ… å›ºå®šé¡ºåº
        pin_memory=True
    )
    
    print(f"[Monitor Set] Using {len(monitor_keys)} fixed val samples for interpretability tracking.")
    
    print(f"\n{'='*70}")
    print(f"Dataset & DataLoader:")
    if args.patch_size is not None:
        print(f"  è®­ç»ƒæ¨¡å¼: PATCHè®­ç»ƒ ({args.patch_size}Ã—{args.patch_size})")
        print(f"  - Train: {len(trainDataset)} patches, batch_size={args.batch_size}")
        print(f"  - Val: {len(valDataset)} å¼ å®Œæ•´å›¾åƒ, batch_size={val_batch_size}")
        print(f"  éªŒè¯æ¨¡å¼: æ»‘çª—é¢„æµ‹ (patch_size={args.patch_size}, overlap=0.5)")
        print(f"  DataLoader: num_workers=4, prefetch_factor=4")
    else:
        print(f"  è®­ç»ƒæ¨¡å¼: å®Œæ•´å›¾åƒè®­ç»ƒï¼ˆä¸æ”¯æŒPPEF++ï¼‰")
        print(f"  Train: {len(trainDataset)} samples")
        print(f"  Val: {len(valDataset)} samples")
    print(f"{'='*70}\n")
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆPPEF++ï¼‰
    print(f"\n{'='*70}")
    print(f"Initializing UNet_PPEF++ model...")
    print(f"{'='*70}\n")
    
    model = UNet_PPEFPP(
        in_channels=40,
        out_channels=1,
        num_prototypes=args.num_prototypes,
        dropout_rate=args.dropout_rate,
        use_spgapp=True,
        use_csse=True,
        use_pgacpp=True
    )
    model.to(device)
    
    # âœ… ä½¿ç”¨ torch.compile() åŠ é€Ÿï¼ˆPyTorch 2.0+ï¼‰
    if args.use_compile:
        try:
            print(f"\n{'='*70}")
            print(f"ğŸš€ å¯ç”¨ torch.compile() åŠ é€Ÿ...")
            print(f"   æ¨¡å¼: reduce-overhead")
            print(f"   é¦–æ¬¡è¿è¡Œä¼šè¿›è¡Œç¼–è¯‘ï¼ˆéœ€è¦ 10-30 ç§’ï¼‰ï¼Œä¹‹åè®­ç»ƒé€Ÿåº¦æå‡ 15-25%")
            print(f"{'='*70}\n")
            model = torch.compile(model, mode='reduce-overhead')
            print(f"âœ… æ¨¡å‹ç¼–è¯‘é…ç½®å®Œæˆï¼\n")
        except Exception as e:
            print(f"âš ï¸  torch.compile() ç¼–è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹: {e}\n")
    else:
        print(f"â„¹ï¸  æœªå¯ç”¨ torch.compile()ï¼ˆå¯é€šè¿‡ --use-compile å¯ç”¨ï¼‰\n")
    
    # åˆ›å»ºæŸå¤±å‡½æ•°ï¼ˆPPEF++ï¼‰
    criterion = PPEFPPLoss(
        lambda_recon=args.lambda_recon,
        lambda_smooth=args.lambda_smooth,
        lambda_div=args.lambda_div,
        lambda_entropy=args.lambda_entropy,
        lambda_orth=args.lambda_orth  # ã€æ–°å¢ã€‘åŸå‹æ­£äº¤çº¦æŸ
    )
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    lf = lambda x: ((1 + np.cos(x * np.pi / args.epochs)) / 2) * (1 - args.lrf) + args.lrf
    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)
    
    # æ··åˆç²¾åº¦
    scaler = GradScaler() if args.amp else None
    if args.amp:
        print(f"âœ“ æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨ (FP16)\n")
    
    # EMAè¿½è¸ªå™¨
    ema_tracker = EMA(decay=0.90)
    
    # è®­ç»ƒæŒ‡æ ‡
    best_ema_dice = 0.0
    train_loss_list = []
    train_seg_loss_list = []  # âœ… æ–°å¢ï¼šè®°å½•è®­ç»ƒæ—¶çš„åˆ†å‰²æŸå¤±ï¼ˆä¸å«æ­£åˆ™åŒ–ï¼‰
    val_loss_list = []
    pseudo_dice_list = []
    ema_dice_list = []
    lr_list = []
    
    # é¢å¤–çš„PPEF++æŸå¤±è®°å½•
    recon_loss_list = []
    smooth_loss_list = []
    div_loss_list = []
    entropy_loss_list = []
    orth_loss_list = []  # ã€æ–°å¢ã€‘åŸå‹æ­£äº¤çº¦æŸæŸå¤±
    
    # ã€æ–°å¢ã€‘ç«¯å…ƒå’Œä¸“å®¶æƒé‡æ¼”åŒ–è¿½è¸ª
    endmember_evolution = []  # å­˜å‚¨ç«¯å…ƒæ¼”åŒ–æ•°æ®
    expert_weights_evolution = []  # å­˜å‚¨ä¸“å®¶æƒé‡æ¼”åŒ–æ•°æ®
    
    print(f"\n{'='*70}")
    print(f"{'å¼€å§‹è®­ç»ƒ UNet_PPEF++':^70}")
    print(f"{'='*70}")
    
    # âœ… torch.compile() æç¤º
    if args.use_compile:
        print(f"\nğŸ’¡ æç¤ºï¼šç”±äºå¯ç”¨äº† torch.compile()ï¼Œ")
        print(f"   ç¬¬ä¸€ä¸ª epoch ä¼šè¿›è¡Œæ¨¡å‹ç¼–è¯‘ï¼ˆå¯èƒ½éœ€è¦é¢å¤– 10-30 ç§’ï¼‰")
        print(f"   ä¹‹åçš„ epoch å°†è·å¾— 15-25% çš„é€Ÿåº¦æå‡ ğŸš€\n")
    
    print()
    
    # æ–°åŠ å†…å®¹
    # ===== è®­ç»ƒå‰ baseline è®°å½•ï¼ˆepoch=0ï¼‰=====
    print("  --> è®°å½•ç«¯å…ƒå’Œä¸“å®¶æƒé‡æ¼”åŒ– (Epoch 0 | before training)...")

    endmember_info = extract_endmember_info(model)
    endmember_info['epoch'] = 0
    endmember_evolution.append(endmember_info)

    avg_w, stat_w = extract_expert_weights_on_monitor(model, monitorLoader, device)
    expert_weights_evolution.append({
        'epoch': 0,
        'expert_weights': avg_w,
        'expert_stats': stat_w,
        'val_dice': None,   # æˆ–è€… 0.0ï¼›åæ­£ epoch0 æ²¡æœ‰ val
    })
    
    # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºåˆå§‹ä¸“å®¶æƒé‡
    if len(avg_w) > 0:
        first_module = list(avg_w.keys())[0]
        weights = avg_w[first_module]
        print(f"      åˆå§‹ä¸“å®¶æƒé‡ ({first_module}): {weights}")

    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        
        # è®­ç»ƒ
        train_loss, train_details, lr = train_one_epoch_ppefpp(
            model, criterion, optimizer, trainLoader, device, scaler, epoch=epoch
        )
        
        scheduler.step()
        
        # éªŒè¯ï¼ˆä½¿ç”¨æ»‘çª—é¢„æµ‹ï¼‰
        if args.patch_size is not None:
            # Patchè®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨æ»‘çª—é¢„æµ‹ï¼ˆ50%é‡å +é«˜æ–¯æƒé‡èåˆï¼‰
            val_loss, val_dice, val_details = calculate_val_loss_and_dice_ppefpp_sliding(
                model, valLoader, device,
                patch_size=args.patch_size,
                overlap=0.5  # 50%é‡å æ»‘çª—
            )
        else:
            # å®Œæ•´å›¾åƒè®­ç»ƒæ¨¡å¼ï¼šç›´æ¥å‰å‘ä¼ æ’­ï¼ˆä¸æ”¯æŒï¼Œå»ºè®®ä½¿ç”¨patchæ¨¡å¼ï¼‰
            raise NotImplementedError("å®Œæ•´å›¾åƒè®­ç»ƒæ¨¡å¼æš‚ä¸æ”¯æŒPPEF++ï¼Œè¯·ä½¿ç”¨--patch-sizeå‚æ•°å¯ç”¨patchè®­ç»ƒ")
        
        # è®¡ç®—epochæ—¶é—´
        epoch_time = time.time() - epoch_start_time
        
        # EMA Dice
        ema_dice = ema_tracker.update('dice', val_dice)
        
        # è®°å½•æŒ‡æ ‡
        train_loss_list.append(train_loss)
        train_seg_loss_list.append(train_details.get('seg', train_loss))  # âœ… æ–°å¢ï¼šè®°å½•è®­ç»ƒåˆ†å‰²æŸå¤±
        val_loss_list.append(val_loss)
        pseudo_dice_list.append(val_dice)
        ema_dice_list.append(ema_dice)
        lr_list.append(lr)
        
        # è®°å½•PPEF++ç‰¹æœ‰çš„æŸå¤±
        recon_loss_list.append(train_details.get('recon', 0.0))
        smooth_loss_list.append(train_details.get('smooth', 0.0))
        div_loss_list.append(train_details.get('div', 0.0))
        entropy_loss_list.append(train_details.get('entropy', 0.0))
        orth_loss_list.append(train_details.get('orth', 0.0))  # ã€æ–°å¢ã€‘
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆåŒ…å«æ—¶é—´ï¼‰
        train_seg_loss = train_details.get('seg', train_loss)
        log_msg = (f"Epoch {epoch+1:4d}/{args.epochs} | "
                  f"train_loss: {train_loss:.4f} (seg: {train_seg_loss:.4f}) | "
                  f"val_loss: {val_loss:.4f} | "
                  f"Dice: {val_dice:.4f} | EMA: {ema_dice:.4f} | lr: {lr:.2e} | "
                  f"{epoch_time:.1f}s")
        
        # æ·»åŠ PPEF++æŸå¤±ä¿¡æ¯
        if 'recon' in train_details:
            log_msg += f" | recon: {train_details['recon']:.4f}"
        
        print(log_msg, flush=True)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if ema_dice > best_ema_dice:
            best_ema_dice = ema_dice
            torch.save(model.state_dict(), f'{fold_dir}/model_best.pth')
            print(f"  --> New best EMA dice: {ema_dice:.4f}")
        
        # å†™å…¥æ—¥å¿—ï¼ˆç§»é™¤flushä»¥å‡å°‘I/Oå¼€é”€ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ç¼“å†²ï¼‰
        with open(f'{fold_dir}/train_log.txt', "a") as f:
            f.write(log_msg + "\n")
        
        # ã€æ–°å¢ã€‘æ¯éš”10ä¸ªepochè®°å½•ç«¯å…ƒå’Œä¸“å®¶æƒé‡æ¼”åŒ–
        if (epoch + 1) % 10 == 0 or epoch == 0 or (epoch + 1) == args.epochs:
            print(f"  --> è®°å½•ç«¯å…ƒå’Œä¸“å®¶æƒé‡æ¼”åŒ– (Epoch {epoch+1})...")
            
            # æå–ç«¯å…ƒä¿¡æ¯
            endmember_info = extract_endmember_info(model)
            endmember_info['epoch'] = epoch + 1
            endmember_evolution.append(endmember_info)
            
            # æå–ä¸“å®¶æƒé‡ï¼ˆåœ¨å›ºå®šMonitor Setä¸Šï¼‰
            avg_w, stat_w = extract_expert_weights_on_monitor(model, monitorLoader, device)
            expert_weights_data = {
                'epoch': epoch + 1,
                'expert_weights': avg_w,
                'expert_stats': stat_w,
                'val_dice': float(val_dice),   # âœ… entropy vs val_dice å¯¹ç…§ç”¨
            }
            expert_weights_evolution.append(expert_weights_data)
            
            # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºä¸“å®¶æƒé‡
            if len(avg_w) > 0:
                first_module = list(avg_w.keys())[0]
                weights = avg_w[first_module]
                print(f"      ä¸“å®¶æƒé‡ ({first_module}): {weights}")
            
            print(f"      ç«¯å…ƒæ­£äº¤æ€§: {endmember_info['orthogonality_score']:.4f}")
        
        # å®šæœŸå¯è§†åŒ–ï¼ˆé™ä½é¢‘ç‡å’ŒDPIä»¥èŠ‚çœæ—¶é—´ï¼‰
        if (epoch + 1) % 20 == 0 or (epoch + 1) == args.epochs:
            # ä¿å­˜è®­ç»ƒæ›²çº¿
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            
            # Lossæ›²çº¿ï¼ˆä¸‰æ¡ï¼šæ€»è®­ç»ƒlossã€è®­ç»ƒåˆ†å‰²lossã€éªŒè¯lossï¼‰
            axes[0, 0].plot(train_loss_list, label='Train Loss (Total)', color='#1f77b4', linewidth=2)
            axes[0, 0].plot(train_seg_loss_list, label='Train Loss (Seg Only)', color='#ff7f0e', linewidth=2, linestyle='--')
            axes[0, 0].plot(val_loss_list, label='Val Loss (Seg Only)', color='#2ca02c', linewidth=2)
            axes[0, 0].set_title('Loss Curves (Train Total vs Seg vs Val)')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].legend(loc='upper right')
            axes[0, 0].grid(True, alpha=0.3)
            
            # Diceæ›²çº¿
            axes[0, 1].plot(pseudo_dice_list, label='Pseudo Dice', alpha=0.6)
            axes[0, 1].plot(ema_dice_list, label='EMA Dice', linewidth=2)
            axes[0, 1].set_title('Dice Score')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
            
            # å­¦ä¹ ç‡
            axes[0, 2].plot(lr_list, color='orange')
            axes[0, 2].set_title('Learning Rate')
            axes[0, 2].grid(True, alpha=0.3)
            
            # PPEF++ç‰¹æœ‰æŸå¤±
            axes[1, 0].plot(recon_loss_list, label='Recon Loss')
            axes[1, 0].set_title('Spectral Reconstruction Loss')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
            
            axes[1, 1].plot(smooth_loss_list, label='Smooth Loss')
            axes[1, 1].plot(entropy_loss_list, label='Entropy Loss')
            axes[1, 1].set_title('Regularization Losses')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
            
            axes[1, 2].plot(div_loss_list, label='Diversity Loss')
            axes[1, 2].plot(orth_loss_list, label='Orthogonality Loss')  # ã€æ–°å¢ã€‘
            axes[1, 2].set_title('Prototype Losses')
            axes[1, 2].legend()
            axes[1, 2].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f'{fold_dir}/training_progress.png', dpi=100)  # é™ä½DPIä»¥åŠ å¿«ä¿å­˜
            plt.close()
            
            # ç«‹å³é‡Šæ”¾å†…å­˜
            import gc
            gc.collect()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), f'{fold_dir}/model_final.pth')
    
    # ã€æ–°å¢ã€‘ç»˜åˆ¶ç«¯å…ƒå’Œä¸“å®¶æƒé‡æ¼”åŒ–å›¾
    print(f"\n{'='*70}")
    print(f"ç”Ÿæˆç«¯å…ƒå’Œä¸“å®¶æƒé‡æ¼”åŒ–å¯è§†åŒ–...")
    print(f"{'='*70}\n")
    
    if len(endmember_evolution) > 0:
        # ç»˜åˆ¶ç«¯å…ƒæ¼”åŒ–å›¾
        plot_endmember_evolution(
            endmember_evolution,
            save_path=f'{fold_dir}/endmember_evolution.png',
            wavelengths=None  # å¦‚æœæœ‰æ³¢é•¿æ–‡ä»¶ï¼Œå¯ä»¥åŠ è½½ï¼šnp.loadtxt('wavelengths.txt')
        )
        
        # ã€æ–°å¢ã€‘ä¿å­˜ç«¯å…ƒæ¼”åŒ–æ•°æ®
        save_endmember_data(endmember_evolution, fold_dir)
    
    if len(expert_weights_evolution) > 0:
        # ç»˜åˆ¶ä¸“å®¶æƒé‡æ¼”åŒ–å›¾
        plot_expert_weights_evolution(
            expert_weights_evolution,
            save_path=f'{fold_dir}/expert_weights_evolution.png'
        )
        
        # ã€æ–°å¢ã€‘ä¿å­˜ä¸“å®¶æƒé‡æ¼”åŒ–æ•°æ®
        save_expert_weights_data(expert_weights_evolution, fold_dir)
        
        # âœ… æ–°å¢ï¼šstats CSV + 3å¼ æ–°å›¾
        print(f"\n{'='*70}")
        print(f"ç”Ÿæˆè§£é‡Šæ€§åˆ†æå›¾è¡¨å’Œæ•°æ®...")
        print(f"{'='*70}")
        
        save_expert_stats_data(expert_weights_evolution, fold_dir)
        
        plot_expert_entropy_evolution(
            expert_weights_evolution,
            save_path=f'{fold_dir}/expert_entropy_evolution.png'
        )
        
        plot_expert_top1_evolution(
            expert_weights_evolution,
            save_path=f'{fold_dir}/expert_top1_evolution.png'
        )
        
        plot_entropy_vs_valdice(
            expert_weights_evolution,
            save_path=f'{fold_dir}/expert_entropy_vs_valdice.png'
        )
    
    print(f"\n{'='*70}")
    print(f"  è®­ç»ƒå®Œæˆï¼")
    print(f"  Best EMA Dice: {best_ema_dice:.4f}")
    print(f"  æ¨¡å‹ä¿å­˜åœ¨: {fold_dir}")
    print(f"\n  å¯è§†åŒ–å›¾è¡¨ï¼š")
    print(f"    - endmember_evolution.png (ç«¯å…ƒæ¼”åŒ–)")
    print(f"    - expert_weights_evolution.png (ä¸“å®¶æƒé‡æ¼”åŒ–)")
    print(f"    - expert_entropy_evolution.png (è·¯ç”±ç†µæ¼”åŒ–)")
    print(f"    - expert_top1_evolution.png (Top-1ä¸“å®¶é€‰æ‹©é¢‘ç‡)")
    print(f"    - expert_entropy_vs_valdice.png (è·¯ç”±ç†µ vs éªŒè¯Diceå¯¹ç…§)")
    print(f"\n  ä¿å­˜çš„æ•°æ®æ–‡ä»¶ï¼š")
    print(f"    ğŸ“Š ç«¯å…ƒæ•°æ®ï¼š")
    print(f"       - endmember_orthogonality.csv (æ­£äº¤æ€§æ¼”åŒ–)")
    print(f"       - endmember_spectra_init_final.csv (åˆå§‹/æœ€ç»ˆå…‰è°±)")
    print(f"       - endmember_correlation_init_final.json (ç›¸å…³æ€§çŸ©é˜µ)")
    print(f"       - endmember_spectra_full.npz (å®Œæ•´æ¼”åŒ–ï¼Œå¯é€‰)")
    print(f"    ğŸ“Š ä¸“å®¶æƒé‡æ•°æ®ï¼š")
    print(f"       - expert_weights_evolution.csv (æƒé‡æ¼”åŒ–)")
    print(f"       - expert_weights_evolution.json (æƒé‡æ¼”åŒ–ï¼ŒJSONæ ¼å¼)")
    print(f"    ğŸ“Š ä¸“å®¶ç»Ÿè®¡æ•°æ®ï¼š")
    print(f"       - expert_entropy_evolution.csv (è·¯ç”±ç†µæ¼”åŒ–)")
    print(f"       - expert_top1_rate_evolution.csv (Top-1ä¸“å®¶é¢‘ç‡)")
    print(f"       - expert_weight_std_evolution.csv (æƒé‡æ ‡å‡†å·®)")
    print(f"{'='*70}\n")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="UNet_PPEF++ Training")
    
    # åŸºç¡€å‚æ•°
    parser.add_argument("--fold", default=0, type=int, help="äº¤å‰éªŒè¯foldç¼–å·")
    parser.add_argument("--batch-size", default=8, type=int, help="Batch size")
    parser.add_argument("--epochs", default=100, type=int, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--lr", default=0.00005, type=float, help="å­¦ä¹ ç‡")
    parser.add_argument("--lrf", default=0.01, type=float, help="æœ€ç»ˆå­¦ä¹ ç‡ç³»æ•°")
    parser.add_argument("--weight-decay", default=3e-5, type=float, help="æƒé‡è¡°å‡")
    parser.add_argument("--amp", action='store_true', help="ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    parser.add_argument("--use-compile", action='store_true', help="ä½¿ç”¨ torch.compile() åŠ é€Ÿï¼ˆPyTorch 2.0+ï¼‰")
    parser.add_argument("--patch-size", default=None, type=int, help="Patchè®­ç»ƒå¤§å°ï¼ˆNone=å®Œæ•´å›¾åƒï¼‰")
    
    # PPEF++ç‰¹æœ‰å‚æ•°
    parser.add_argument("--num-prototypes", default=4, type=int, help="åŸå‹æ•°é‡")
    parser.add_argument("--dropout-rate", default=0.1, type=float, help="Dropoutæ¦‚ç‡")
    parser.add_argument("--lambda-recon", default=0.1, type=float, help="é‡å»ºæŸå¤±æƒé‡")
    parser.add_argument("--lambda-smooth", default=0.01, type=float, help="å¹³æ»‘æŸå¤±æƒé‡")
    parser.add_argument("--lambda-div", default=0.01, type=float, help="å¤šæ ·æ€§æŸå¤±æƒé‡")
    parser.add_argument("--lambda-entropy", default=0.001, type=float, help="ç†µæŸå¤±æƒé‡")
    parser.add_argument("--lambda-orth", default=0.01, type=float, help="åŸå‹æ­£äº¤çº¦æŸæƒé‡")  # ã€æ–°å¢ã€‘
    
    args = parser.parse_args()
    print(args)
    
    main(args)

